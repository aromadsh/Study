{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 문서 요약 (Text Summarization)\n",
    "\n",
    "## 9-0 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 국가 차원의 빅데이터 활용 시대가 열린다. 새로운 산업 창출과 기존 산업의 변화에 이르는\n",
      "를 혁신하고 기업의 경쟁력을 한 단계 제고할 수 있도록 정책적 역량을 집중하겠다”고 밝혔다\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def cleansing(soup):\n",
    "    body_content = soup.select_one(\"#articleBodyContents\")\n",
    "    content = body_content.get_text()\n",
    "    for strong in body_content.select(\"strong\"):\n",
    "        content = content.replace(strong.get_text(), \"\")\n",
    "\n",
    "    for td in body_content.select(\"td\"):\n",
    "        content = content.replace(td.get_text(), \"\")\n",
    "    \n",
    "    content = re.sub(r\"\\[[가-힣 ]+\\]\", \"\", content)\n",
    "    start_pos = re.search(r\"[가-힣]{2,4}\\s\\(?\\w+@\\w+\\.\\w+(.\\w+)?\\)\", content).start()\n",
    "    content = content[:start_pos-1]\n",
    "    content = content.replace(\"\\n\", \"\").replace(\"// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\", \"\")\n",
    "\n",
    "\n",
    "    return content\n",
    "\n",
    "def get_news_by_url(url):\n",
    "    headers={\"user-agent\":\"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    content = cleansing(soup)\n",
    "    return content\n",
    "\n",
    "doc = get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108')\n",
    "print(doc[:50])\n",
    "print(doc[-50:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-1 Luhn Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss in c:\\users\\rnru1\\anaconda3\\envs\\abangues\\lib\\site-packages (2.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from kss import split_sentences\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "# 문장 분리\n",
    "def get_sentences(txt):\n",
    "    return split_sentences(txt)\n",
    "#     return sent_tokenize(txt)\n",
    "# 토큰화\n",
    "def get_words(txt):\n",
    "    return [token[0] for token in mecab.pos(txt) if token[1][0] == \"N\" and len(token[0]) >1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 중요 단어 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'사과', '포도'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어(토큰)의 가중치 계산 및 범위에 포함되는 토큰 식별 \n",
    "def get_keywords(word_list , min_ratio=0.001, max_ratio=0.5) :\n",
    "    assert (min_ratio < 1 and max_ratio < 1)\n",
    "    \n",
    "    # 토큰별로 빈도수 카운팅\n",
    "    count_dict = {}\n",
    "        \n",
    "    for word in word_list :\n",
    "#         if word in count_dict.keys():\n",
    "#             count_dict[word] += 1\n",
    "#         else :\n",
    "#             count_dict[word] = 1\n",
    "        count_dict.setdefault(word, 0)\n",
    "        count_dict[word] += 1\n",
    "    \n",
    "# 분석 문서의 총 토큰수 대비 해당 토큰의 빈도 비율\n",
    "    keywords = set()\n",
    "    for word, cnt in count_dict.items():\n",
    "        word_percentage = cnt / len(word_list)\n",
    "        \n",
    "         # 사전 정의한 비율내에 포함 된 경우 키워드에 추가\n",
    "        if word_percentage >=min_ratio and word_percentage <= max_ratio:\n",
    "            keywords.add(word)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "get_keywords(['바나나','사과','바나나','바나나','포도'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 문장 중요도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 가중치 계산\n",
    "def get_sentence_weight (sentence , keywords):\n",
    "    tokens = sentence.split(\" \") # 문장을 받고 리스트가 되고\n",
    "    window_start, window_end = 0, -1\n",
    "    \n",
    "    # 문장내에서 윈도 시작 위치 탐색\n",
    "    # 범위내 속한 키워드가 등장하는 첫번째 위치 계산\n",
    "    for i in range(len(tokens)):\n",
    "        if sentence[i] in keywords: # keywords는 set() 값!\n",
    "            window_start = i\n",
    "            break\n",
    "    \n",
    "    # 문장내에서 윈도 종료 위치 탐색\n",
    "    # 범위내 속한 키워드가 등장하는 마지막 위치 계산\n",
    "    for i in range(len(tokens)-1, 0, -1):\n",
    "        if tokens[i] in keywords:\n",
    "            window_end = i \n",
    "            break\n",
    "    \n",
    "    # 윈도의 시작위치가 종료위치보다 큰경우 => 분석할 단어(토큰)가 없는 경우 종료\n",
    "    if window_start > window_end:\n",
    "        return 0\n",
    "    \n",
    "       # 윈도 크기 계산\n",
    "    window_size = window_end - window_start + 1\n",
    "\n",
    "    \n",
    "    # 분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수 카운팅\n",
    "    keyword_cnt = 0\n",
    "    for w in tokens[window_start : window_start+ window_size]:\n",
    "        if w in keywords:\n",
    "            keyword_cnt += 1\n",
    "    \n",
    "    # (분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수) / 윈도사이즈\n",
    "    return  keyword_cnt * keyword_cnt * 1.0 / float(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 요약\n",
    "def summarize(content ,max_no_of_sentences = 10):\n",
    "    \n",
    "    \n",
    "    # 단어(토큰) 분리\n",
    "    word_list = get_words(content)\n",
    "    \n",
    "    # 단어(토큰) 가중치 계산 및 범위 내 포함 단어(토큰) 추출\n",
    "    keywords = get_keywords(word_list)\n",
    "\n",
    "    # 문장별 가중치 계산\n",
    "    sentence_list = get_sentences(content)\n",
    "    sentence_weight = []\n",
    "    \n",
    "    for sentence in sentence_list :\n",
    "        sentence_weight.append((get_sentence_weight(sentence, keywords), sentence))\n",
    "        \n",
    "           \n",
    "    # 문장별 가중치 역순 계산\n",
    "    sentence_weight.sort(reverse=True)\n",
    "#     print(sentence_weight)\n",
    "    \n",
    "    \n",
    "    return [ weight[1] for weight in sentence_weight[:max_no_of_sentences]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 ‘빅데이터 플랫폼 및 센터’ 출범식 행사를 개최했다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "금융 플랫폼의 경우 소상공인 신용평가 고도화 등을 통해 금융 취약 계층 대상 중금리 대출이자를 2%p 절감해 연간 1조원의 신규대출을 창출할 전망이다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "빅데이터는 데이터 활용을 통해 혁신성장을 이루자는 문재인 정부의 경제 성장 핵심 요소중 하나다.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "li = summarize(doc, 3)\n",
    "for s in li:\n",
    "    print(s)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Text rank 직접 구현하기 (Matrix 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 자카드 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"딸기 바나나 사과 파인애플. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "# 문장간 유사도 측정 (자카드 유사도 사용)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1 = [token[0] for token in mecab.pos(sentence1) if token[1][0] in [\"N\", \"V\"]]\n",
    "    sentence2 = [token[0] for token in mecab.pos(sentence2) if token[1][0] in [\"N\", \"V\"]]\n",
    "    \n",
    "    union = set(sentence1).union(set(sentence2))\n",
    "    intersection = set(sentence1).intersection(set(sentence2))\n",
    "    \n",
    "    return len(intersection)/len(union)\n",
    "  \n",
    "\n",
    "sentence_similarity('나는 치킨을 좋아해','나는 치킨을 싫어해')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나\n",
      "치킨\n",
      "좋아해\n"
     ]
    }
   ],
   "source": [
    "for token in mecab.pos(\"나는 치킨을 좋아해\") :\n",
    "    if token[1][0] in [\"N\", \"V\"]:\n",
    "        print(token[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.3409091 , 0.11363637, 0.54545456],\n",
       "       [0.45454544, 0.        , 0.        , 0.54545456],\n",
       "       [1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.57142854, 0.4285714 , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def buildMatrix(sentences):\n",
    "    score = np.ones(len(sentences), dtype=np.float32)\n",
    "    weighted_edge = np.zeros((len(sentences), len(sentences)), dtype=np.float32)\n",
    "\n",
    "    # 문장별로 그래프 edge를 Matrix 형태로 생성\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            weighted_edge[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
    "\n",
    "    # normalize \n",
    "    for i in range(len(weighted_edge)):\n",
    "        score[i] = weighted_edge[i].sum()\n",
    "        weighted_edge[i] /= score[i]\n",
    "    \n",
    "    return  weighted_edge\n",
    "\n",
    "\n",
    "Text = \"딸기 바나나 사과 파인애플 수박. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\"\n",
    "buildMatrix(sent_tokenize(Text))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 문장 중요도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(weight_edge, score, threshold=0.0001, d=0.85, max_iter = 50):\n",
    "    for iter in range(max_iter):\n",
    "        new_score = (1-d) + d *  weight_edge.T.dot(score)\n",
    "\n",
    "        if abs(new_score - score).sum() <= threshold:\n",
    "            break\n",
    "    \n",
    "        score = new_score\n",
    "    \n",
    "    return new_score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def summarize(text, n=10):\n",
    "    \n",
    "#   sentences = sent_tokenize(text)\n",
    "    sentences = get_sentences(text)\n",
    "  \n",
    "    weighted_edge = buildMatrix(sentences)\n",
    "    init_score = np.ones(len(sentences), dtype=np.float32)\n",
    "    score = scoring(weighted_edge, init_score)\n",
    "\n",
    "    sorted_score = sorted(enumerate(score), key=lambda x : x[1], reverse=True)[:n]\n",
    "    return [ sentences[sent[0]] for sent in sorted_score ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('딸기 바나나 사과 파인애플.', 0.3430375429643529)\n",
      "('파인애플 사과 딸기 바나나.', 0.3430375429643529)\n",
      "('바나나 사과 딸기 포도.', 0.2663058284544005)\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(Text, 3)\n",
    "\n",
    "for sent in summary :\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이런 맥락 속에서 빅데이터센터는 공공과 민간이 협업해 활용도 높은 양질의 데이터를 생산·구축하고, 플랫폼은 이를 수집·분석·유통하는 역할을 담당한다.\n",
      "100개 센터에서 수집된 데이터를 융합·분석한 뒤 맞춤형 데이터 제작 등 양질의 데이터로 재생산하고, 기업들이 필요로 하는 데이터를 원하는 형태로 즉시 활용할 수 있도록 제공할 계획이다.\n",
      "센터와 플랫폼 간 연계체계에는 민간 클라우드를 기반으로 활용하고, 센터에 축적된 데이터도 계속 외부와 개방·공유하며 최신·연속성을 확보한다는 계획이다.\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(doc, 3)\n",
    "\n",
    "for sent in summary :\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 TextRank 직접 구현하기 (Graph 활용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = \"딸기 바나나 사과 파인애플. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rnru1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 자카드 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "# 문장간 유사도 측정 (자카드 유사도 사용)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1 = [token[0] for token in mecab.pos(sentence1) if token[1][0] in [\"N\", \"V\"]]\n",
    "    sentence2 = [token[0] for token in mecab.pos(sentence2) if token[1][0] in [\"N\", \"V\"]]\n",
    "    \n",
    "    union = set(sentence1).union(set(sentence2))\n",
    "    intersection = set(sentence1).intersection(set(sentence2))\n",
    "    \n",
    "    return len(intersection)/len(union)\n",
    "  \n",
    "\n",
    "sentence_similarity('나는 치킨을 좋아해','나는 치킨을 싫어해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(nodes):\n",
    "    return [(start, end, sentence_similarity(start, end)) for start in nodes for end in nodes if start is not end] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딸기 바나나 사과 파인애플 바나나 사과 딸기 포도\n",
      "딸기 바나나 사과 파인애플 복숭아 수박\n",
      "딸기 바나나 사과 파인애플 파인애플 사과 딸기 바나나\n",
      "바나나 사과 딸기 포도 딸기 바나나 사과 파인애플\n",
      "바나나 사과 딸기 포도 복숭아 수박\n",
      "바나나 사과 딸기 포도 파인애플 사과 딸기 바나나\n",
      "복숭아 수박 딸기 바나나 사과 파인애플\n",
      "복숭아 수박 바나나 사과 딸기 포도\n",
      "복숭아 수박 파인애플 사과 딸기 바나나\n",
      "파인애플 사과 딸기 바나나 딸기 바나나 사과 파인애플\n",
      "파인애플 사과 딸기 바나나 바나나 사과 딸기 포도\n",
      "파인애플 사과 딸기 바나나 복숭아 수박\n"
     ]
    }
   ],
   "source": [
    "nodes = [\"딸기 바나나 사과 파인애플\", \"바나나 사과 딸기 포도\", \"복숭아 수박\", \"파인애플 사과 딸기 바나나\"]\n",
    "for start in nodes :\n",
    "    for end in nodes :\n",
    "        if start is not end:\n",
    "            print(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def rank(nodes,edges):\n",
    "    graph=nx.diamond_graph()\n",
    "    graph.clear() \n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_weighted_edges_from(edges)\n",
    "\n",
    "    return nx.pagerank(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, num_summaries=3):\n",
    "    nodes = sentences(text)\n",
    "    edges = connect(nodes)\n",
    "    scores = rank(nodes, edges)\n",
    "    return sorted(scores.items(), key=lambda x : x[1], reverse=True)[:num_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('딸기 바나나 사과 파인애플.', 0.3430375429643529)\n",
      "('파인애플 사과 딸기 바나나.', 0.3430375429643529)\n",
      "('바나나 사과 딸기 포도.', 0.2663058284544005)\n"
     ]
    }
   ],
   "source": [
    "summary=summarize(Text, 3)\n",
    "\n",
    "for sent in summary :\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨 인코딩인 경우 미니배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.718987110506905"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[2, 1]])\n",
    "t = np.array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "print(t.size)\n",
    "print(y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-7\n",
    "    return (f(x+h) - f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-7\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999989472883"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent_line(f,x):\n",
    "    d = numerical_diff(f,x)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(function_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-7\n",
    "    \n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h)\n",
    "        x[idx] = float(tmp_val) +h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h)\n",
    "        x[idx] = float(tmp_val) -h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.99999998, 8.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        \n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x, np.array(x_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 100\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814392e-10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.00000000e+00,  4.00000000e+00],\n",
       "       [-2.40000000e+00,  3.20000000e+00],\n",
       "       [-1.92000000e+00,  2.56000000e+00],\n",
       "       [-1.53600000e+00,  2.04800000e+00],\n",
       "       [-1.22880000e+00,  1.63840000e+00],\n",
       "       [-9.83040001e-01,  1.31072000e+00],\n",
       "       [-7.86432000e-01,  1.04857600e+00],\n",
       "       [-6.29145601e-01,  8.38860801e-01],\n",
       "       [-5.03316480e-01,  6.71088641e-01],\n",
       "       [-4.02653184e-01,  5.36870913e-01],\n",
       "       [-3.22122547e-01,  4.29496730e-01],\n",
       "       [-2.57698038e-01,  3.43597384e-01],\n",
       "       [-2.06158430e-01,  2.74877907e-01],\n",
       "       [-1.64926744e-01,  2.19902326e-01],\n",
       "       [-1.31941395e-01,  1.75921861e-01],\n",
       "       [-1.05553116e-01,  1.40737489e-01],\n",
       "       [-8.44424931e-02,  1.12589991e-01],\n",
       "       [-6.75539945e-02,  9.00719927e-02],\n",
       "       [-5.40431956e-02,  7.20575942e-02],\n",
       "       [-4.32345565e-02,  5.76460753e-02],\n",
       "       [-3.45876452e-02,  4.61168603e-02],\n",
       "       [-2.76701161e-02,  3.68934882e-02],\n",
       "       [-2.21360929e-02,  2.95147906e-02],\n",
       "       [-1.77088743e-02,  2.36118325e-02],\n",
       "       [-1.41670995e-02,  1.88894660e-02],\n",
       "       [-1.13336796e-02,  1.51115728e-02],\n",
       "       [-9.06694365e-03,  1.20892582e-02],\n",
       "       [-7.25355492e-03,  9.67140657e-03],\n",
       "       [-5.80284394e-03,  7.73712526e-03],\n",
       "       [-4.64227515e-03,  6.18970021e-03],\n",
       "       [-3.71382012e-03,  4.95176017e-03],\n",
       "       [-2.97105610e-03,  3.96140813e-03],\n",
       "       [-2.37684488e-03,  3.16912651e-03],\n",
       "       [-1.90147590e-03,  2.53530120e-03],\n",
       "       [-1.52118072e-03,  2.02824096e-03],\n",
       "       [-1.21694458e-03,  1.62259277e-03],\n",
       "       [-9.73555662e-04,  1.29807422e-03],\n",
       "       [-7.78844529e-04,  1.03845937e-03],\n",
       "       [-6.23075624e-04,  8.30767499e-04],\n",
       "       [-4.98460499e-04,  6.64613999e-04],\n",
       "       [-3.98768399e-04,  5.31691199e-04],\n",
       "       [-3.19014719e-04,  4.25352959e-04],\n",
       "       [-2.55211775e-04,  3.40282368e-04],\n",
       "       [-2.04169420e-04,  2.72225894e-04],\n",
       "       [-1.63335536e-04,  2.17780715e-04],\n",
       "       [-1.30668429e-04,  1.74224572e-04],\n",
       "       [-1.04534743e-04,  1.39379658e-04],\n",
       "       [-8.36277946e-05,  1.11503726e-04],\n",
       "       [-6.69022357e-05,  8.92029810e-05],\n",
       "       [-5.35217885e-05,  7.13623848e-05],\n",
       "       [-4.28174308e-05,  5.70899078e-05],\n",
       "       [-3.42539447e-05,  4.56719262e-05],\n",
       "       [-2.74031557e-05,  3.65375410e-05],\n",
       "       [-2.19225246e-05,  2.92300328e-05],\n",
       "       [-1.75380197e-05,  2.33840262e-05],\n",
       "       [-1.40304157e-05,  1.87072210e-05],\n",
       "       [-1.12243326e-05,  1.49657768e-05],\n",
       "       [-8.97946607e-06,  1.19726214e-05],\n",
       "       [-7.18357285e-06,  9.57809715e-06],\n",
       "       [-5.74685828e-06,  7.66247772e-06],\n",
       "       [-4.59748663e-06,  6.12998217e-06],\n",
       "       [-3.67798930e-06,  4.90398574e-06],\n",
       "       [-2.94239144e-06,  3.92318859e-06],\n",
       "       [-2.35391315e-06,  3.13855087e-06],\n",
       "       [-1.88313052e-06,  2.51084070e-06],\n",
       "       [-1.50650442e-06,  2.00867256e-06],\n",
       "       [-1.20520353e-06,  1.60693805e-06],\n",
       "       [-9.64162827e-07,  1.28555044e-06],\n",
       "       [-7.71330262e-07,  1.02844035e-06],\n",
       "       [-6.17064210e-07,  8.22752280e-07],\n",
       "       [-4.93651368e-07,  6.58201824e-07],\n",
       "       [-3.94921094e-07,  5.26561459e-07],\n",
       "       [-3.15936875e-07,  4.21249167e-07],\n",
       "       [-2.52749500e-07,  3.36999334e-07],\n",
       "       [-2.02199600e-07,  2.69599467e-07],\n",
       "       [-1.61759680e-07,  2.15679574e-07],\n",
       "       [-1.29407744e-07,  1.72543659e-07],\n",
       "       [-1.03526195e-07,  1.38034927e-07],\n",
       "       [-8.28209562e-08,  1.10427942e-07],\n",
       "       [-6.62567650e-08,  8.83423534e-08],\n",
       "       [-5.30054120e-08,  7.06738827e-08],\n",
       "       [-4.24043296e-08,  5.65391062e-08],\n",
       "       [-3.39234637e-08,  4.52312849e-08],\n",
       "       [-2.71387709e-08,  3.61850280e-08],\n",
       "       [-2.17110168e-08,  2.89480224e-08],\n",
       "       [-1.73688134e-08,  2.31584179e-08],\n",
       "       [-1.38950507e-08,  1.85267343e-08],\n",
       "       [-1.11160406e-08,  1.48213874e-08],\n",
       "       [-8.89283246e-09,  1.18571100e-08],\n",
       "       [-7.11426597e-09,  9.48568797e-09],\n",
       "       [-5.69141277e-09,  7.58855037e-09],\n",
       "       [-4.55313022e-09,  6.07084030e-09],\n",
       "       [-3.64250418e-09,  4.85667224e-09],\n",
       "       [-2.91400334e-09,  3.88533779e-09],\n",
       "       [-2.33120267e-09,  3.10827023e-09],\n",
       "       [-1.86496214e-09,  2.48661619e-09],\n",
       "       [-1.49196971e-09,  1.98929295e-09],\n",
       "       [-1.19357577e-09,  1.59143436e-09],\n",
       "       [-9.54860615e-10,  1.27314749e-09],\n",
       "       [-7.63888492e-10,  1.01851799e-09]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "러닝 레이트가 많은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.25899015e+09, -3.00990351e+09])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])  \n",
    "x, x_history = gradient_descent(function_2, init_x, lr=10, step_num=step_num)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "러닝 레이트가 적은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])  \n",
    "x, x_history = gradient_descent(function_2, init_x, lr=1e-10, step_num=100)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단층 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    if exp_x.ndim == 1:\n",
    "        sum_exp_x = sum(exp_x)\n",
    "    else : \n",
    "        sum_exp_x = np.sum(exp_x, axis=1).reshape(-1,1)\n",
    "        \n",
    "    y = exp_x / sum_exp_x\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-7\n",
    "    \n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #f(x+h)\n",
    "        x[idx] = float(tmp_val) +h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h)\n",
    "        x[idx] = float(tmp_val) -h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f,X)\n",
    "    \n",
    "    else :\n",
    "        grad = np.zeros_like(X)\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f,x)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43466416, -0.15487517,  0.25438857],\n",
       "       [-1.99497187, -0.21689668,  0.91822127]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "net.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.05627318, -0.28813212,  0.97903229])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.192604692574866"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13493801, -0.21875922,  1.35369723],\n",
       "       [-1.70240702, -0.32813882,  2.03054585]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(W): # 더미 라고 하셨음 좀더 봐야할듯....ㅎ.ㅎ.ㅎ.ㅎ.\n",
    "    return net.loss(x, t)\n",
    "dw = numerical_gradient(f, net.W)\n",
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13493801, -0.21875922,  1.35369723],\n",
       "       [-1.70240702, -0.32813882,  2.03054585]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw = numerical_gradient(lambda w: net.loss(x, t), net.W) # 람다로 풀어씀\n",
    "dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 여기서 스탑! ㅋㅋㅋㅋㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.W -= 0.001 * dw # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.05406005, -0.28770554,  0.97639258])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.182310520692416"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이층 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "\n",
    "        return T\n",
    "\n",
    "    with open('mnist.pkl', 'rb') as f: #알아서 학습이 되는건가? f\n",
    "        dataset = pickle.load(f)\n",
    "        \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    \n",
    "    delta = 1e-7\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(t==y)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(50, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(network.params['W1'].shape)\n",
    "print(network.params['b1'].shape)\n",
    "print(network.params['W2'].shape)\n",
    "print(network.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(50, 784)\n",
    "y = network.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0] # train size 60000개 였음\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 2.293731225914733 train acc, test acc | 0.0993, 0.1032\n",
      "loss : 2.2996903783646845 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.292340040851765 train acc, test acc | 0.11541666666666667, 0.1161\n",
      "loss : 2.285913872126021 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2791471298649912 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.296933364578808 train acc, test acc | 0.0993, 0.1032\n",
      "loss : 2.298418623143078 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.299797718116416 train acc, test acc | 0.13615, 0.1398\n",
      "loss : 2.2942399306124095 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.295197667751185 train acc, test acc | 0.1095, 0.103\n",
      "loss : 2.294721851853108 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2792807843160063 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2935873239689677 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.281353336285354 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2890881684523885 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.3020165265438037 train acc, test acc | 0.10218333333333333, 0.101\n",
      "loss : 2.2942919404791238 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2885315912225885 train acc, test acc | 0.09751666666666667, 0.0974\n",
      "loss : 2.2816681308577573 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2985156764827614 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2804579754120984 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.307560163722981 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.292045112788549 train acc, test acc | 0.197, 0.1989\n",
      "loss : 2.2861797527626107 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2917912270907492 train acc, test acc | 0.09871666666666666, 0.098\n",
      "loss : 2.2933613349445663 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2962378070980676 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.300428083864768 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.296618458022781 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.29403643634663 train acc, test acc | 0.11381666666666666, 0.1147\n",
      "loss : 2.2752700702539994 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2996539118836195 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2825667724072116 train acc, test acc | 0.14598333333333333, 0.1457\n",
      "loss : 2.279373757435847 train acc, test acc | 0.09915, 0.1009\n",
      "loss : 2.285713226605692 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2897709756783464 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2844726563743447 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2950866138165917 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2801182074910797 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.270737313444422 train acc, test acc | 0.19551666666666667, 0.1958\n",
      "loss : 2.2901335910511085 train acc, test acc | 0.18288333333333334, 0.1845\n",
      "loss : 2.295585366551488 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.274534291826654 train acc, test acc | 0.10566666666666667, 0.1071\n",
      "loss : 2.299286442495913 train acc, test acc | 0.11328333333333333, 0.1137\n",
      "loss : 2.291664250905226 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.281796950334477 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.293787971850026 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.3055164175833824 train acc, test acc | 0.12128333333333334, 0.1223\n",
      "loss : 2.2864808092470748 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.292234034617063 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2822923035181266 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2806674988361872 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.279593947131619 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.3059508495303827 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.29149243376906 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2863028819923694 train acc, test acc | 0.10441666666666667, 0.1028\n",
      "loss : 2.2777365716944806 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2957801859973883 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.271060000023359 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2828443387846424 train acc, test acc | 0.11236666666666667, 0.1135\n",
      "loss : 2.2892269804207217 train acc, test acc | 0.17556666666666668, 0.1774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-2e24bdb01b28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"W1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"W2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-188872d8a681>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-f177db610214>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, X)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_numerical_gradient_no_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-1f5e980c7a8f>\u001b[0m in \u001b[0;36m_numerical_gradient_no_batch\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#f(x-h)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mfxh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-188872d8a681>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mloss_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-188872d8a681>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-188872d8a681>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    train_acc = network.accuracy(x_train, t_train)\n",
    "    test_acc = network.accuracy(x_test, t_test)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    \n",
    "    print(f\"loss : {loss} train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
